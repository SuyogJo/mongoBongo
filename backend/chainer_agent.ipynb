{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGbJSwCSQ8In"
      },
      "source": [
        "# Content Generation Pipeline\n",
        "### Briefly:\n",
        "### Step 1: Main Content Generation Pipeline\n",
        "* Use Online Research Agent\n",
        "* Generates initial Painpoints, Descriptions, Sources, Links, Ideas, Features\n",
        "* GPT-3.5-turbo\n",
        "\n",
        "### Step 2: Writing Improvement Piepline\n",
        "* Use Offline Prompting (no agent)\n",
        "* Improves technical writng of content\n",
        "* Putting content in desired style, length and rought formatting\n",
        "* Keeps original link\n",
        "* GPT-4-turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uru1yuCDQ8In"
      },
      "source": [
        "Follow https://www.youtube.com/watch?v=ogQUlS7CkYA&t=789s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d573HIXQ8In"
      },
      "source": [
        "# Step 1: Prepareation & Initilization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-KhHCbnQ8In"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install langchain\n",
        "  !pip install scrapingant-client\n",
        "  !pip install beautifulsoup4\n",
        "  !pip install openai\n",
        "  !pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-KCNhw-hHFm",
        "outputId": "ff287af5-ac83-4a30-c2d4-c495246e3fea"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.34)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.45)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: scrapingant-client in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from scrapingant-client) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install scrapingant-client\n",
        "!pip install beautifulsoup4\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7CJlOKRYCiw",
        "outputId": "d166a116-8e01-4bbb-8514-de99a33fed62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1 langsmith-0.1.49 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting scrapingant-client\n",
            "  Downloading scrapingant_client-2.0.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from scrapingant-client) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->scrapingant-client) (2024.2.2)\n",
            "Installing collected packages: scrapingant-client\n",
            "Successfully installed scrapingant-client-2.0.1\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting openai\n",
            "  Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.2\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZE6KpcF7Q8In"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "from langchain_community.chat_models import ChatPerplexity\n",
        "from langchain_community.chat_models import ChatPerplexity\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, AIMessage\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate ,PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.tools import BaseTool\n",
        "from typing import Type\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "# from dotenv import load_dotenv\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.prompts import MessagesPlaceholder\n",
        "\n",
        "from scrapingant_client import ScrapingAntClient\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL8skzfYQ8Io"
      },
      "source": [
        "### API KEYS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "xHrsaSIzX7g4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jaWIMK2VQ8Io"
      },
      "outputs": [],
      "source": [
        "serper_key = userdata.get(\"SERPER_API_KEY\")\n",
        "scraping_ant_key = userdata.get (\"SCRAPING_ANT_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQRCAbAyQ8Io"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xYNZkugQQ8Io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ee86bb-0ac5-4d82-bd28-831cb63a3857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "openai_key =  userdata.get(\"OPENAI_API_KEY\")\n",
        "pplx_key =  userdata.get(\"PERPLEXITY_API_KEY\")\n",
        "\n",
        "\n",
        "gpt3 = ChatOpenAI(temperature=0, openai_api_key=openai_key,model= \"gpt-3.5-turbo-0125\") # one way to load a model\n",
        "gpt4 = ChatOpenAI(temperature=0, openai_api_key=openai_key,model= \"gpt-4-0125-preview\") # one way to load a model\n",
        "pplx = ChatPerplexity(temperature=0.1, pplx_api_key=pplx_key, model=\"sonar-medium-online\") # or sonar-small-onlinesd\n",
        "pplx_sm = ChatPerplexity(temperature=0.5, pplx_api_key=pplx_key, model=\"sonar-small-online\") # or sonar-small-onlinesd\n",
        "mixtr  = ChatPerplexity (temperature=0 ,pplx_api_key=pplx_key, model='mistral-7b-instruct')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PPLX to get links to Scrpe given brand name"
      ],
      "metadata": {
        "id": "VO9nL4_fkKY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trends= \"\"\n",
        "\n",
        "# prompt_templete = \"\"\"\n",
        "# Given the input of characteristics of a person,\n",
        "# and combine the scaped data about google trends: {trends}\n",
        "# output the top 5 topics that would be interesting for this\n",
        "# person to read.\n",
        "# \"\"\"\n",
        "\n",
        "# t = ChatPromptTemplate.from_messages (\n",
        "#      [\n",
        "#          (\"human\", prompt_templete )\n",
        "#      ]\n",
        "#  )\n",
        "# pipe = t | pplx\n",
        "\n",
        "# res = pipe.invoke ({\"trends\": trends})\n",
        "# print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CkQpnJZkQHS",
        "outputId": "3ea9df90-eeb2-4000-c049-98a6e86bd103"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To output the top 5 topics that would be interesting for a person to read based on their characteristics and combined with the scraped data about Google Trends, we first need to define the person's characteristics. However, the provided search results do not include any information about a specific person.\n",
            "\n",
            "Therefore, I will provide the top 5 trending searches of 2023, which can be found in the first search result:\n",
            "\n",
            "1. **Wordle**: A popular word guessing game that took the internet by storm.\n",
            "2. **India vs England**: A cricket match between India and England that captured the world's attention.\n",
            "3. **Ukraine**: The ongoing conflict and humanitarian crisis in Ukraine.\n",
            "4. **Queen Elizabeth**: The passing of Queen Elizabeth II and her impact on the world.\n",
            "5. **Ind vs SA**: A cricket match between India and South Africa.\n",
            "\n",
            "These topics cover a range of interests, including gaming, sports, current events, and history. They are likely to be of interest to a broad audience, as they reflect some of the most searched topics of the year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuHueIOrkKLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVY0GCoVQ8Io"
      },
      "source": [
        "# Step 2: Main Content Generation Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxSST2CiQ8Io"
      },
      "source": [
        "### Prompts - System Message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i8gYAmYQ8Io"
      },
      "source": [
        "In the current pipeline, p_system_message is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EMpi9oPRQ8Io"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# p_system_message_webscrape =  \"\"\"\n",
        "#   ROLE:\n",
        "#     You are a world class analyzer of trends on the internet.\n",
        "#     Who can identify the relevant trends that matches the characteristics of a person using the search and scrape_website.\n",
        "#     Analyze the google trends for the last week and come up with top 5 most relevant trends pertaining to the characteristics of the person.\n",
        "#     Output the result in the below FORMAT:\n",
        "#     Topic Name: xxxx\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# t_query_list = \"\"\"\n",
        "# {query}\n",
        "# OUTPUT FORMAT FOR EACH ITEM IDENTIFIED\n",
        "# Topic Name: xxxx\n",
        "\n",
        "\n",
        "\n",
        "# The final output should be Product name:, Price, Link for each prodcut Identified\n",
        "# \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyYRbCMSQ8Io"
      },
      "source": [
        "### Building Aegnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zB0paHfQ8Io"
      },
      "source": [
        "#### 1. Search Tool -> Using Serper API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cqVw9TCzQ8Io"
      },
      "outputs": [],
      "source": [
        "# Serper\n",
        "def search (query):\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "\n",
        "    payload = json.dumps({\n",
        "    \"q\":query\n",
        "    })\n",
        "    headers = {\n",
        "    'X-API-KEY': serper_key,\n",
        "    'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "    print(response.text)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7TiAoZHQ8Io"
      },
      "source": [
        "### 2. Scraping Tool + Summary Tool\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_PDC3ThQ8Ip"
      },
      "source": [
        "https://github.com/ScrapingAnt/scrapingant-client-python?tab=readme-ov-file#sending-custom-headers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIp5YqgsQ8Ip"
      },
      "source": [
        "main scrape_website function"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3LX_szPdIMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z4acg4IqQ8Ip"
      },
      "outputs": [],
      "source": [
        "def scrape_website (objective: str, url: str):\n",
        "    print (\"**** Scraping Website **** \")\n",
        "\n",
        "    client =  ScrapingAntClient(token=scraping_ant_key)\n",
        "\n",
        "    headers = {\n",
        "        'Cache-Control': 'no-cache',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    res = client.general_request(\n",
        "            url=url,\n",
        "            headers=headers\n",
        "    )\n",
        "\n",
        "    if res.status_code == 200:\n",
        "        soup = BeautifulSoup (res.content,\"html.parser\")\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # print (\"text output:\" ,text)\n",
        "        print (\"*** Finished website Scrape  ***\")\n",
        "\n",
        "\n",
        "        ###### SUMMARY CODE, NOT USED CURRENTLY #######\n",
        "        if len(text) >10000:\n",
        "            (\"*** Long text, entering summary function ***\")\n",
        "            out = summary (objective,text)\n",
        "            (\"*** Summary Finished***\")\n",
        "            return out\n",
        "        else:\n",
        "\n",
        "            (\"*** Finishd Scraping Function ***\")\n",
        "            return text\n",
        "    else:\n",
        "        print (f\"HTTP request failed with status code {res.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5lJyqy2Q8Ip"
      },
      "source": [
        "Summary Function -> map reduce summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JZZ2-UEkQ8Ip"
      },
      "outputs": [],
      "source": [
        "def summary (objective,content):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        separators=['\\n\\n',\"\\n\"], chunk_size=10000,chunk_overlap=500)\n",
        "    docs  = text_splitter.create_documents([content])\n",
        "    print (\"** received text inputing into ma_prompt\")\n",
        "    map_prompt = \"\"\"\n",
        "    Write a summary of the following text for {objective}:\n",
        "    \"{text}\"\n",
        "\n",
        "    *** SUMMARY ***:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    map_prompt_template = PromptTemplate (\n",
        "        template=map_prompt, input_variables=[\"text\",\"objective\"]\n",
        "    )\n",
        "    # print (\"** objective **: \", objective)\n",
        "    # print (\"** map prompt template: ** \",map_prompt_template)\n",
        "\n",
        "    # print  (\"*** building chain ***\")\n",
        "    summary_chain = load_summarize_chain(\n",
        "        llm=gpt3,\n",
        "        chain_type='map_reduce',\n",
        "        map_prompt=map_prompt_template,\n",
        "        combine_prompt=map_prompt_template,\n",
        "        verbose =True\n",
        "    )\n",
        "    (\" *** Invokings summary chain **** \")\n",
        "    out = summary_chain.invoke ({\"input_documents\":docs,\"objective\": objective})\n",
        "    (\" *** Finished summary chain **** \")\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e-Y3aLMQ8Ip"
      },
      "source": [
        "Predefined formatting classes XX NOT USED -> good for documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VKEs-k1bQ8Ip"
      },
      "outputs": [],
      "source": [
        "# define input\n",
        "class ScrapeWebsiteInput (BaseModel):\n",
        "    objective: str = Field (\n",
        "        description=\"the objective $ task that users gives to the agent\")\n",
        "    url: str = Field (description=\" the url of the website to be scraped \")\n",
        "\n",
        "# define function\n",
        "class ScrapeWebsiteTool (BaseTool):\n",
        "    name = \"scrape_website\"\n",
        "    description =\"\"\"useful when you need to get data from a website url,\n",
        "    passing both url and objective to the function;\n",
        "    DO NOT Make up any URL!the url should only be from the search results\"\"\"\n",
        "    args_schema: Type[BaseModel] = ScrapeWebsiteInput\n",
        "\n",
        "    def _run (self,objective: str ,url:str):\n",
        "        return scrape_website(objective,url)\n",
        "\n",
        "    def _arun(self,url:str):\n",
        "        raise NotImplementedError(f\"Error in implementing the function : {url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG8V3hhtQ8Ip"
      },
      "source": [
        "### 3. Create Agent Painpoint -> Can Wrap in Function\n",
        "Note: for the agent_llm gpt 3 seems to invoke the best results so far -> because it actually invokes a search where as pplx online model doesn't.\n",
        "Still need to test out different models. esp.gpt4\n",
        "Although gpt3 is lacking in the task in generating painpoint titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mDh6VvX0Q8Ip"
      },
      "outputs": [],
      "source": [
        "# tool list for the agents to use\n",
        "\n",
        "def create_main_content_agent (system_message = p_system_message_webscrape, memory_llm=gpt3, llm=gpt3):\n",
        "\n",
        "    tools = [\n",
        "        Tool (\n",
        "            name = \"Search\",\n",
        "            func= search,\n",
        "            description = \"\"\"\n",
        "            useful when you need to answer questions about current\n",
        "            events,data. You should ask targetted questions\n",
        "            \"\"\"\n",
        "        ),\n",
        "        ScrapeWebsiteTool(),\n",
        "    ]\n",
        "    system_message = SystemMessage (content=system_message)\n",
        "\n",
        "    agent_kwargs = {\n",
        "        \"extract_prompt_messages\" : [MessagesPlaceholder(variable_name=\"memory\")],\n",
        "        \"system_message\" : system_message\n",
        "    }\n",
        "\n",
        "    memory = ConversationBufferMemory (\n",
        "        memory_key=\"memory\",\n",
        "        return_messages=True,\n",
        "        llm=memory_llm,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    # initialize agent\n",
        "    agent = initialize_agent (\n",
        "        tools,\n",
        "        llm=llm,\n",
        "        agent=AgentType.OPENAI_FUNCTIONS,\n",
        "        verbose=True,\n",
        "        agent_kwargs=agent_kwargs,\n",
        "        memory=memory,\n",
        "    )\n",
        "    return agent\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wyXYmk9HTlNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTBtFYtbTlgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8bixZfHQ8Ip"
      },
      "source": [
        "### Implmentation\n",
        "Looping through Shorter but diverse prompt selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_character ():\n",
        "\n",
        "\n",
        "  from google.colab import userdata\n",
        "  from openai import OpenAI\n",
        "  import os\n",
        "  openai_key = userdata.get('OPENAI_API_KEY')\n",
        "  client = OpenAI (\n",
        "      api_key=openai_key,\n",
        "      )\n",
        "  img_path = \"https://fmehyvjvxhlvozschibx.supabase.co/storage/v1/object/public/Images/IMG_7738_2.jpg\"\n",
        "\n",
        "  def generate_description(img_path, human_prompt, system_prompt, detail='low'):\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                  {\"type\": \"text\", \"text\": human_prompt},\n",
        "                  {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                      \"url\": img_path,\n",
        "                    },\n",
        "                  },\n",
        "                ],\n",
        "            },\n",
        "\n",
        "\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    description = res.choices[0].message.content\n",
        "    return description\n",
        "  s = \"\"\" you are good at telling a persons characteristics based on their image\"\"\"\n",
        "  h = \"\"\" describe this persons appearance, percived characteristics in descriptive yet concise manner\"\"\"\n",
        "  return generate_description (img_path, h, s)"
      ],
      "metadata": {
        "id": "KGfEfCcfg4ZD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person = get_character()"
      ],
      "metadata": {
        "id": "Iy0KhnN8g9zI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F_-SJFr-g4Ht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "YqeDrm_4Q8Ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2603603-6a66-43fc-bf59-32b8af0586ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The person in the image is a young adult male with a relaxed and approachable demeanor. He has dark hair, styled casually, and a soft, neutral expression that conveys a sense of calm confidence. His clothing is casual professional, with a dark brown zip-up jacket over a black shirt, and light gray pants, which together present a practical yet tidy appearance. The office environment with visible tech workstations in the background hints at a professional setting, suggesting he might be involved in a corporate or tech-driven field. His posture is upright, suggesting attentiveness and readiness.']\n"
          ]
        }
      ],
      "source": [
        "### This one is currently used -> can modify ###\n",
        "query_list = [\n",
        "person\n",
        "]\n",
        "\n",
        "\n",
        "print(query_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1rnCYHnQ8Ip"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_skKZbKQ8Ip"
      },
      "source": [
        "Put it in a function -> automating getting painpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bg8vRe92Q8Ip"
      },
      "outputs": [],
      "source": [
        "main_content_temp_list = []\n",
        "improved_content_temp_list= []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "p_system_message_webscrape =  \"\"\"\n",
        "  ROLE:\n",
        "    You are a world class analyzer of trends on the internet.\n",
        "    Who can identify the relevant trends that matches the characteristics of a person using the search and scrape_website.\n",
        "    You will be provided with a summary of a user's characteristics or personalities,\n",
        "    based on this user input, first to see what are popular google trends this week,\n",
        "    and match top 5 topics based on this person's personalities, and also\n",
        "    for each topic, give it a short summary, and resons why this person will be interested in this topic.\n",
        "\n",
        "    Output the result in the below JSON FORMAT:\n",
        "    Topic1 Name: xxxx\n",
        "    Summary: xxxx\n",
        "    Reasons why I would be interesed:  xxxx\n",
        "    Topic2 Name: xxxx\n",
        "    Summary: xxxx\n",
        "    Reasons why I would be interesed:  xxxx\n",
        "    Topic3 Name: xxxx\n",
        "    Summary: xxxx\n",
        "    Reasons why I would be interesed:  xxxx\n",
        "\n",
        "    Topic4 Name: xxxx\n",
        "    Summary: xxxx\n",
        "    Reasons why I would be interesed:  xxxx\n",
        "\n",
        "    Topic5 Name: xxxx\n",
        "    Summary: xxxx\n",
        "    Reasons why I would be interesed:  xxxx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "t_query_list = \"\"\"\n",
        "{query}\n",
        "OUTPUT FORMAT FOR EACH ITEM IDENTIFIED\n",
        "Topic Name: xxxx\n",
        "Summary: xxxx\n",
        "Reasons why I would be interesed:  xxxx\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "0ecuKuxecMI3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EaE5Mv3zQ8Ip"
      },
      "outputs": [],
      "source": [
        "# Main function for getting the pointpoints\n",
        "def generate_main_content(query_list:list,main_content_temp_list:list,iterations:int,memory_llm=gpt3):\n",
        "\n",
        "    # setting a cap for safety purposes\n",
        "    if iterations > 20:\n",
        "        print (\"This is too many iterations. returning without running code\")\n",
        "        return\n",
        "\n",
        "    if iterations > len (query_list):\n",
        "        print (f\"your input iterations is more than the prompts in the query list, only running code for {len(query_list)} iterations\")\n",
        "    l = query_list[:iterations]\n",
        "\n",
        "\n",
        "    # Create Agent\n",
        "    agent = create_main_content_agent(memory_llm=memory_llm)\n",
        "\n",
        "\n",
        "    for i, q in enumerate(l) :\n",
        "\n",
        "        ## need to document this formatting\n",
        "        empty_query = SystemMessagePromptTemplate.from_template(t_query_list)\n",
        "        filled_query =empty_query.format(query = q)\n",
        "\n",
        "        out = agent.invoke({\"input\": str (filled_query)}) # actual call\n",
        "\n",
        "        print (f'*** {i + 1} iteration finished ***')\n",
        "\n",
        "        main_content_temp_list.append(out[\"output\"])\n",
        "\n",
        "    n = iterations * 5\n",
        "    print (f\"*** Sucessfully generated {n} Content ***\")\n",
        "\n",
        "    # Get today's date in the format YYYY-MM-DD\n",
        "    today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Construct the filename with today's date\n",
        "        # Create directory if it doesn't exist\n",
        "    dir_name = 'raw_main_content'\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "    # Define the path for the output file\n",
        "    file_path = os.path.join(dir_name, f'main_content_{today_date}.txt')\n",
        "\n",
        "    # Writing the pain points to the text file\n",
        "    with open(file_path, 'w') as file:\n",
        "        for content in main_content_temp_list:\n",
        "            file.write(content + '\\n')  # Write each pain point on a new line\n",
        "\n",
        "    print(f\"*** Content saved to '{file_path}' ***\")\n",
        "    return main_content_temp_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTC1ddH8Q8Iq",
        "outputId": "3e58bba6-0721-4c9d-8210-be5c801d4fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Search` with `google trends last week`\n",
            "\n",
            "\n",
            "\u001b[0m{\"searchParameters\":{\"q\":\"google trends last week\",\"type\":\"search\",\"engine\":\"google\"},\"organic\":[{\"title\":\"Weekly Trends\",\"link\":\"https://trends.google.com/trends/story/US_cu_27NlPHEBAADPDM_en\",\"snippet\":\"Weekly Trends · Searches for “How to use clippers to cut hair” have gone up by 140% this week · Searches for “Puppy adoption near me” went up 100% · Some of us ...\",\"position\":1},{\"title\":\"Daily Search Trends - Google Trends\",\"link\":\"https://trends.google.com/trends/trendingsearches/daily\",\"snippet\":\"Trends ; 1. Dickey Betts · The Associated Press ; 2. Tortured Poets Department · NBC News ; 3. Arizona Coyotes · Sportico.com ; 4. Columbia University · The Washington ...\",\"sitelinks\":[{\"title\":\"Trends\",\"link\":\"https://trends.google.com/trends/trendingsearches/realtime?geo=IN&category=all\"},{\"title\":\"Daily\",\"link\":\"https://trends.google.com/trends/trendingsearches/daily?geo=IN&hl=en-GB\"},{\"title\":\"Notifications Subscriptions\",\"link\":\"https://trends.google.com/trends/subscriptions?geo=US&hl=en-US\"}],\"position\":2},{\"title\":\"Trending Searches - Google Trends\",\"link\":\"https://trends.google.as/trends/trendingsearches/daily?geo=US&hl=en-US\",\"snippet\":\"1. Mandisa · NPR ; 2. thanK you aIMee · Billboard ; 3. Ryan Garcia · CBS sports.com ; 4. Man set himself on fire · The New York Times ; 5. The Alchemy Taylor Swift.\",\"position\":3},{\"title\":\"Google Trends\",\"link\":\"https://trends.google.la/trends/trendingsearches/daily?geo=US\",\"snippet\":\"Trends ; 1. Taylor Swift · The Associated Press ; 2. Israel · CBS News ; 3. Allman Brothers · NPR ; 4. Florida communism bill · The Independent Florida Alligator ; 5.\",\"position\":4},{\"title\":\"Year in Search 2023 - Google\",\"link\":\"https://about.google/intl/ALL_us/stories/year-in-search/\",\"snippet\":\"Look back on 25 years in Search history, honoring the most searched moments that have inspired the world and the next generation to come.\",\"position\":5},{\"title\":\"See what's trending across Google Search, Google News and ...\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/fundamentals/google-trends-lesson/\",\"snippet\":\"Google Trends allows you to see the topics people are—or aren't—following, practically in real time. Journalists can use this information to explore ...\",\"position\":6},{\"title\":\"FAQ about Google Trends data\",\"link\":\"https://support.google.com/trends/answer/4365533?hl=en\",\"snippet\":\"Real-time data is a sample covering the last seven days. ... While only a sample of Google searches are used in Google Trends, this is sufficient because we ...\",\"position\":7},{\"title\":\"Explore the world's searches with the new Google Trends\",\"link\":\"https://blog.google/products/search/google-trends-new-design/\",\"snippet\":\"The Google Trends site has a fresh new look, so it's easier than ever to find the trend data you're looking for.\",\"date\":\"Mar 8, 2023\",\"position\":8},{\"title\":\"Advanced Google Trends\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/google-trends/advanced-google-trends/\",\"snippet\":\"Looking to become a Google Trends expert? This lesson will demonstrate some tips and tricks to get more precise and compelling results from Trends Explore.\",\"sitelinks\":[{\"title\":\"Storytelling with Google Trends\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/storytelling-with-google-trends/\"},{\"title\":\"Google Earth Pro, Maps and...\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/google-historical-imagery-google-earth-maps-and-timelapse/\"},{\"title\":\"Basics\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/basics-of-google-trends/\"}],\"position\":9}],\"peopleAlsoAsk\":[{\"question\":\"How do I find recent Trends on Google?\",\"snippet\":\"The Google Trends homepage (google.com/trends) features clustered topics that Google detects are related and trending together on either Search, Google News, or YouTube.\",\"title\":\"See what's trending across Google Search, Google News and ...\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/fundamentals/google-trends-lesson/\"},{\"question\":\"How far back can Google Trends go?\",\"snippet\":\"There are two samples of Google Trends data that can be accessed: Real-time data is a sample covering the last seven days. Non-realtime data is a separate sample from real-time data and goes as far back as 2004 and up to 72 hours before your search.\",\"title\":\"FAQ about Google Trends data\",\"link\":\"https://support.google.com/trends/answer/4365533?hl=en\"},{\"question\":\"How do I read Google Trends interest over time?\",\"snippet\":\"When you search for a term on Trends, you'll see a graph showing the term's popularity over time in (nearly) real time. Hovering your mouse over the graph reveals a number, which reflects how many searches have been done for the particular term relative to the total number of searches done on Google.\",\"title\":\"Google Trends: Understanding the data.\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/google-trends-understanding-the-data/\"},{\"question\":\"How do I get Google Trends data?\",\"snippet\":\"EXPORT, EMBED, AND CITE TRENDS DATA\\n1\\nOpen Google Trends.\\n2\\nSearch for a term.\\n3\\nIn the top right of the chart, click Download .\\n4\\nOpen the file using a spreadsheet application, like Google Sheets.\",\"title\":\"Export, embed, and cite Trends data - Google Help\",\"link\":\"https://support.google.com/trends/answer/4365538?hl=en\"}],\"relatedSearches\":[{\"query\":\"Google trends last week today\"},{\"query\":\"Google trends last week usa\"},{\"query\":\"Google trends last week worldwide\"},{\"query\":\"Google trends last week youtube\"},{\"query\":\"Top Google searches today\"},{\"query\":\"Google Trends worldwide\"},{\"query\":\"Google Trends 2023\"},{\"query\":\"Google Trends YouTube\"}]}\n",
            "\u001b[36;1m\u001b[1;3m{\"searchParameters\":{\"q\":\"google trends last week\",\"type\":\"search\",\"engine\":\"google\"},\"organic\":[{\"title\":\"Weekly Trends\",\"link\":\"https://trends.google.com/trends/story/US_cu_27NlPHEBAADPDM_en\",\"snippet\":\"Weekly Trends · Searches for “How to use clippers to cut hair” have gone up by 140% this week · Searches for “Puppy adoption near me” went up 100% · Some of us ...\",\"position\":1},{\"title\":\"Daily Search Trends - Google Trends\",\"link\":\"https://trends.google.com/trends/trendingsearches/daily\",\"snippet\":\"Trends ; 1. Dickey Betts · The Associated Press ; 2. Tortured Poets Department · NBC News ; 3. Arizona Coyotes · Sportico.com ; 4. Columbia University · The Washington ...\",\"sitelinks\":[{\"title\":\"Trends\",\"link\":\"https://trends.google.com/trends/trendingsearches/realtime?geo=IN&category=all\"},{\"title\":\"Daily\",\"link\":\"https://trends.google.com/trends/trendingsearches/daily?geo=IN&hl=en-GB\"},{\"title\":\"Notifications Subscriptions\",\"link\":\"https://trends.google.com/trends/subscriptions?geo=US&hl=en-US\"}],\"position\":2},{\"title\":\"Trending Searches - Google Trends\",\"link\":\"https://trends.google.as/trends/trendingsearches/daily?geo=US&hl=en-US\",\"snippet\":\"1. Mandisa · NPR ; 2. thanK you aIMee · Billboard ; 3. Ryan Garcia · CBS sports.com ; 4. Man set himself on fire · The New York Times ; 5. The Alchemy Taylor Swift.\",\"position\":3},{\"title\":\"Google Trends\",\"link\":\"https://trends.google.la/trends/trendingsearches/daily?geo=US\",\"snippet\":\"Trends ; 1. Taylor Swift · The Associated Press ; 2. Israel · CBS News ; 3. Allman Brothers · NPR ; 4. Florida communism bill · The Independent Florida Alligator ; 5.\",\"position\":4},{\"title\":\"Year in Search 2023 - Google\",\"link\":\"https://about.google/intl/ALL_us/stories/year-in-search/\",\"snippet\":\"Look back on 25 years in Search history, honoring the most searched moments that have inspired the world and the next generation to come.\",\"position\":5},{\"title\":\"See what's trending across Google Search, Google News and ...\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/fundamentals/google-trends-lesson/\",\"snippet\":\"Google Trends allows you to see the topics people are—or aren't—following, practically in real time. Journalists can use this information to explore ...\",\"position\":6},{\"title\":\"FAQ about Google Trends data\",\"link\":\"https://support.google.com/trends/answer/4365533?hl=en\",\"snippet\":\"Real-time data is a sample covering the last seven days. ... While only a sample of Google searches are used in Google Trends, this is sufficient because we ...\",\"position\":7},{\"title\":\"Explore the world's searches with the new Google Trends\",\"link\":\"https://blog.google/products/search/google-trends-new-design/\",\"snippet\":\"The Google Trends site has a fresh new look, so it's easier than ever to find the trend data you're looking for.\",\"date\":\"Mar 8, 2023\",\"position\":8},{\"title\":\"Advanced Google Trends\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/google-trends/advanced-google-trends/\",\"snippet\":\"Looking to become a Google Trends expert? This lesson will demonstrate some tips and tricks to get more precise and compelling results from Trends Explore.\",\"sitelinks\":[{\"title\":\"Storytelling with Google Trends\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/storytelling-with-google-trends/\"},{\"title\":\"Google Earth Pro, Maps and...\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/google-historical-imagery-google-earth-maps-and-timelapse/\"},{\"title\":\"Basics\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/basics-of-google-trends/\"}],\"position\":9}],\"peopleAlsoAsk\":[{\"question\":\"How do I find recent Trends on Google?\",\"snippet\":\"The Google Trends homepage (google.com/trends) features clustered topics that Google detects are related and trending together on either Search, Google News, or YouTube.\",\"title\":\"See what's trending across Google Search, Google News and ...\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/fundamentals/google-trends-lesson/\"},{\"question\":\"How far back can Google Trends go?\",\"snippet\":\"There are two samples of Google Trends data that can be accessed: Real-time data is a sample covering the last seven days. Non-realtime data is a separate sample from real-time data and goes as far back as 2004 and up to 72 hours before your search.\",\"title\":\"FAQ about Google Trends data\",\"link\":\"https://support.google.com/trends/answer/4365533?hl=en\"},{\"question\":\"How do I read Google Trends interest over time?\",\"snippet\":\"When you search for a term on Trends, you'll see a graph showing the term's popularity over time in (nearly) real time. Hovering your mouse over the graph reveals a number, which reflects how many searches have been done for the particular term relative to the total number of searches done on Google.\",\"title\":\"Google Trends: Understanding the data.\",\"link\":\"https://newsinitiative.withgoogle.com/resources/trainings/google-trends-understanding-the-data/\"},{\"question\":\"How do I get Google Trends data?\",\"snippet\":\"EXPORT, EMBED, AND CITE TRENDS DATA\\n1\\nOpen Google Trends.\\n2\\nSearch for a term.\\n3\\nIn the top right of the chart, click Download .\\n4\\nOpen the file using a spreadsheet application, like Google Sheets.\",\"title\":\"Export, embed, and cite Trends data - Google Help\",\"link\":\"https://support.google.com/trends/answer/4365538?hl=en\"}],\"relatedSearches\":[{\"query\":\"Google trends last week today\"},{\"query\":\"Google trends last week usa\"},{\"query\":\"Google trends last week worldwide\"},{\"query\":\"Google trends last week youtube\"},{\"query\":\"Top Google searches today\"},{\"query\":\"Google Trends worldwide\"},{\"query\":\"Google Trends 2023\"},{\"query\":\"Google Trends YouTube\"}]}\u001b[0m\u001b[32;1m\u001b[1;3mTopic Name: How to use clippers to cut hair\n",
            "\n",
            "Topic Name: Puppy adoption near me\n",
            "\n",
            "Topic Name: Dickey Betts\n",
            "\n",
            "Topic Name: Tortured Poets Department\n",
            "\n",
            "Topic Name: Arizona Coyotes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*** 1 iteration finished ***\n",
            "*** Sucessfully generated 5 Content ***\n",
            "*** Content saved to 'raw_main_content/main_content_2024-04-21.txt' ***\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Based on the Google Trends search results, here are the top 5 most relevant trends for you:\\n\\nTopic Name: Trending Searches\\nTopic Name: 2022 Shopping Trends\\nTopic Name: Year in Search 2023\\nTopic Name: Google Trends: What It Is & How to Use the Data for SEO\\nTopic Name: Basics of Google Trends',\n",
              " 'The top 5 most relevant trends from Google Trends last week are as follows:\\n\\nTopic Name: How to use clippers to cut hair\\nTopic Name: Puppy adoption near me\\nTopic Name: Mandisa\\nTopic Name: thanK you aIMee\\nTopic Name: Ryan Garcia',\n",
              " 'The top 5 most relevant trends from Google Trends last week are:\\n\\nTopic Name: How to use clippers to cut hair\\nTopic Name: Puppy adoption near me\\nTopic Name: Mandisa\\nTopic Name: thanK you aIMee\\nTopic Name: Ryan Garcia',\n",
              " 'The top 5 most relevant trends from Google Trends last week are as follows:\\n\\nTopic Name: How to use clippers to cut hair\\nTopic Name: Puppy adoption near me\\nTopic Name: Mandisa\\nTopic Name: thanK you aIMee\\nTopic Name: Ryan Garcia',\n",
              " 'Here are the top 5 most relevant trends from Google Trends last week:\\n\\nTopic Name: How to use clippers to cut hair\\nTopic Name: Puppy adoption near me\\nTopic Name: Mandisa\\nTopic Name: thanK you aIMee\\nTopic Name: Ryan Garcia',\n",
              " 'Based on the Google Trends for the last week, here are the top 5 most relevant trends for you:\\n\\nTopic Name: How to use clippers to cut hair\\nSummary: Searches for “How to use clippers to cut hair” have gone up by 140% this week.\\nReasons why I would be interested: Since you like active things and urban fashion, learning how to use clippers to cut hair could be a useful skill for grooming and styling.\\n\\nTopic Name: Puppy adoption near me\\nSummary: Searches for “Puppy adoption near me” went up 100%.\\nReasons why I would be interested: If you enjoy active activities, having a pet like a puppy can add joy and companionship to your life.\\n\\nTopic Name: Taylor Swift\\nSummary: Taylor Swift was trending in searches.\\nReasons why I would be interested: Taylor Swift is not only a popular musician but also known for her fashion sense, which aligns with your interest in urban fashion.\\n\\nTopic Name: Mandisa\\nSummary: Mandisa was trending in searches.\\nReasons why I would be interested: Mandisa is a well-known singer and could be of interest to you if you enjoy music and entertainment.\\n\\nTopic Name: Ryan Garcia\\nSummary: Ryan Garcia was trending in searches.\\nReasons why I would be interested: Ryan Garcia is a professional boxer, and if you like active things, following his career and matches could be engaging for you.',\n",
              " \"Based on the Google Trends data, here are the top 5 most relevant trends for the last week:\\n\\nTopic Name: Trending Searches\\nSummary: Trending searches on Google Trends including topics like Mandisa, thanK you aIMee, Ryan Garcia, and more.\\nReasons why I would be interested: This trend can provide insights into popular search queries and trending topics that are currently capturing people's attention. It can be a great way to stay updated on what's happening in various fields and industries.\",\n",
              " \"Based on the Google Trends for the last week, here are the top 5 most relevant trends pertaining to the characteristics of the person in the image:\\n\\n1. Topic Name: How to use clippers to cut hair\\nSummary: Searches for “How to use clippers to cut hair” have gone up by 140% this week.\\nReasons why I would be interested: This trend aligns with the neat and professional style of the individual in the image, indicating a focus on grooming and personal appearance.\\n\\n2. Topic Name: Puppy adoption near me\\nSummary: Searches for “Puppy adoption near me” went up 100%.\\nReasons why I would be interested: This trend reflects a compassionate and approachable side, suggesting an interest in pets and animal welfare.\\n\\n3. Topic Name: Taylor Swift\\nSummary: Taylor Swift has been trending in searches.\\nReasons why I would be interested: This trend may indicate a shared interest in music and popular culture, adding a touch of creativity to the individual's profile.\\n\\n4. Topic Name: Arizona Coyotes\\nSummary: Arizona Coyotes have been trending in searches.\\nReasons why I would be interested: This trend could suggest a potential interest in sports or a connection to the Arizona region, adding a dynamic aspect to the individual's personality.\\n\\n5. Topic Name: Mandisa\\nSummary: Mandisa has been trending in searches.\\nReasons why I would be interested: This trend may indicate a diverse taste in music or a curiosity about trending artists, showcasing a well-rounded and open-minded nature.\",\n",
              " \"Here are the top 5 most relevant trends from Google Trends last week:\\n\\nTopic Name: How to use clippers to cut hair\\nSummary: Searches for “How to use clippers to cut hair” have gone up by 140% this week.\\nReasons why I would be interested: This trend could be relevant for someone interested in grooming and hairstyling tips, which might appeal to the person's practical and tidy appearance.\\n\\nTopic Name: Puppy adoption near me\\nSummary: Searches for “Puppy adoption near me” went up 100%.\\nReasons why I would be interested: This trend could be appealing to someone with a relaxed and approachable demeanor, hinting at a potential interest in pets and animal welfare.\\n\\nTopic Name: Mandisa\\nSummary: Mandisa was a trending search term.\\nReasons why I would be interested: This trend could be of interest to someone who enjoys music or is a fan of Mandisa, reflecting a potential interest in entertainment.\\n\\nTopic Name: Ryan Garcia\\nSummary: Ryan Garcia was a trending search term.\\nReasons why I would be interested: This trend could be relevant for someone interested in sports or boxing, indicating a potential interest in physical activities or sports.\\n\\nTopic Name: Taylor Swift\\nSummary: Taylor Swift was a trending search term.\\nReasons why I would be interested: This trend could be appealing to someone who enjoys music or is a fan of Taylor Swift, reflecting a potential interest in entertainment and pop culture.\",\n",
              " 'Topic Name: How to use clippers to cut hair\\n\\nTopic Name: Puppy adoption near me\\n\\nTopic Name: Dickey Betts\\n\\nTopic Name: Tortured Poets Department\\n\\nTopic Name: Arizona Coyotes']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "generate_main_content(query_list,main_content_temp_list,1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_followup =  \"\"\"\n",
        "you are an expert webscraper\n",
        "given the a  list of products PRODUCTS and their meta data: name, price, links, use the search and scrape_website tool\n",
        "to scrape for more information about the products using the links.\n",
        "\n",
        "Output:\n",
        "Product Name:\n",
        "Price:\n",
        "Link (to the specific product page):\n",
        "\n",
        "USER WILL INPUT PRODUCTS, THEN YOU ANSWER\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ousC3rNA3Yfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the contents of a text file into a string.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        str: The contents of the text file as a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "ZVKIwsTG_qRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = str (load_file ('raw_main_content/main_content_2024-04-20.txt'))"
      ],
      "metadata": {
        "id": "qhW66EARAUtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function for getting the pointpoints\n",
        "def generate_followup_content(main_content_temp_list:list,iterations:int,memory_llm=gpt3):\n",
        "\n",
        "    f = str (load_file ('raw_main_content/main_content_2024-04-20.txt'))\n",
        "    q = f\n",
        "\n",
        "    ## need to document this formatting\n",
        "\n",
        "\n",
        "    agent2 = create_main_content_agent(memory_llm=memory_llm,system_message =p_followup)\n",
        "\n",
        "    out = agent2.invoke({\"input\": str (q)}) # actual call\n",
        "\n",
        "\n",
        "    # main_content_temp_list.append(out[\"output\"])\n",
        "\n",
        "\n",
        "    # # Get today's date in the format YYYY-MM-DD\n",
        "    # today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # # Construct the filename with today's date\n",
        "    #     # Create directory if it doesn't exist\n",
        "    # dir_name = 'raw_main_content'\n",
        "    # if not os.path.exists(dir_name):\n",
        "    #     os.makedirs(dir_name)\n",
        "\n",
        "    # # Define the path for the output file\n",
        "    # file_path = os.path.join(dir_name, f'main_content_{today_date}.txt')\n",
        "\n",
        "    # # Writing the pain points to the text file\n",
        "    # with open(file_path, 'w') as file:\n",
        "    #     for content in main_content_temp_list:\n",
        "    #         file.write(content + '\\n')  # Write each pain point on a new line\n",
        "\n",
        "    # print(f\"*** Content saved to '{file_path}' ***\")\n",
        "    # return main_content_temp_list\n",
        "\n"
      ],
      "metadata": {
        "id": "BGmWOT5A4pqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BddQjp9D_nTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_followup_content(main_content_temp_list,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-oyzTFP4mGx",
        "outputId": "bc7503a0-f026-4abc-b902-131b9931cdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mProduct Name:\n",
            "Price: \n",
            "Link (to the specific product page):\n",
            "\n",
            "1. **GG Marmont small shoulder bag**\n",
            "   - Price: \n",
            "   - Link: [Gucci Handbags](https://www.gucci.com/us/en/ca/women/handbags-c-women-handbags)\n",
            "\n",
            "2. **Gucci Luce small shoulder bag**\n",
            "   - Price: \n",
            "   - Link: [Gucci Handbags](https://www.gucci.com/us/en/ca/women/handbags-c-women-handbags)\n",
            "\n",
            "3. **Gucci Moon Side mini shoulder bag**\n",
            "   - Price: \n",
            "   - Link: [Gucci Handbags](https://www.gucci.com/us/en/ca/women/handbags-c-women-handbags)\n",
            "\n",
            "4. **GG Marmont shoulder bag with crystals**\n",
            "   - Price: \n",
            "   - Link: [Gucci Handbags](https://www.gucci.com/us/en/ca/women/handbags-c-women-handbags)\n",
            "\n",
            "5. **Gucci Bags for Women - FARFETCH US**\n",
            "   - Price Range: $850 to $3,400\n",
            "   - Link: [Gucci Bags on Farfetch](https://www.farfetch.com/shopping/women/gucci/bags-purses-1/items.aspx)\n",
            "\n",
            "6. **New In: Women's Handbags - GUCCI® US**\n",
            "   - Price Range: $1,490 to $3,790\n",
            "   - Link: [New Gucci Handbags](https://www.gucci.com/us/en/ca/whats-new/new-in/this-week-women/handbags-c-new-women-handbags)\n",
            "\n",
            "7. **Gucci bags for Women - SSENSE**\n",
            "   - Price Range: $790 to $4,500\n",
            "   - Link: [Gucci Bags on SSENSE](https://www.ssense.com/en-us/women/designers/gucci/bags)\n",
            "\n",
            "8. **Gucci Bags for Women - NET-A-PORTER**\n",
            "   - Price Range: $1,400 to $3,980\n",
            "   - Link: [Gucci Bags on Net-a-Porter](https://www.net-a-porter.com/en-us/shop/designer/gucci/bags)\n",
            "\n",
            "9. **Gucci Bags on SaksFifthAvenue**\n",
            "   - Price Range: $450 to $1,350\n",
            "   - Link: [Gucci Bags on SaksFifthAvenue](https://www.saksfifthavenue.com/brand/gucci)\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMtN7VKP4o_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isE-OOz6Q8Iq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5wxmzQDQ8Iq"
      },
      "source": [
        "## Step 3. Writng Improvment Pipeline (offline) -> Currently Used\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSq0Fcl3Q8Iq"
      },
      "source": [
        "### Prompt - System Message\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yntreTMDQ8Iq"
      },
      "outputs": [],
      "source": [
        "# ## *** This is the one currently in use *** ##\n",
        "# p_system_message_improve_writing = \"\"\"\n",
        "#     You are a As a seasoned technical blog writer, you excel in demystifying complex tech concepts with clarity.\n",
        "\n",
        "#     Your default writing style is:\n",
        "#     - Your content is rich with insights, examples, and a touch of humor, making technology accessible and engaging.\n",
        "#     Stay ahead of tech trends, and blend professionalism with personality to educate, inspire, and entertain your audience\n",
        "\n",
        "#     If the user specifices your WRITING STYLE, output content in their exact WRITING STYLE\n",
        "\n",
        "#     the user will input some data, you are able to write content exactly as they tell you to. and output in the exact format they tell you to.\n",
        "#     and output the data in the same length as they tell you to. Below is the data.\n",
        "#     {input_data}.\n",
        "#     wait for the users input and write content for them.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# # p_system_message_ideas =  \"\"\"\n",
        "# # You are a world-class researcher  who can do detail research on\n",
        "# # any topic by combining the context of {input_data}, as well as searching the internet,\n",
        "# # and produce facts based results using the search and scrape_website.\n",
        "# # you do not make things up nor hallucinate you will try as hard as possible to gather facts and data to back up the research\n",
        "\n",
        "# # Please make sure you complete the objective above with the following rules :\n",
        "\n",
        "# # 1. You should do enough research to gather as much information as possible about the objective\n",
        "# # 2. If there are URL of the relevant links and articles, you will scrape it together more information\n",
        "# # 3. After researching, you should think “is there any new things I should search and scrape based on the data.\n",
        "# # I  collected to increase research quality?”if the answer is yes. Continue; but don't do this for more than 3 iterations don’t do this more than 3 iterations\n",
        "# # 4. IMPORTANT: you should not make things up nor hallucinate, you should only write facts and data that you have gathered\n",
        "\n",
        "# # \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoHvL911Q8Iq"
      },
      "outputs": [],
      "source": [
        "# def improve_writing_ol(improved_content_temp_list:list, main_content_temp_list: list, llm = gpt4):\n",
        "#     for i, pp_batch in enumerate (main_content_temp_list):  # need to add to this\n",
        "#         count  = 1\n",
        "#         # TODO: run ai\n",
        "#         # q = \"\"\"\n",
        "#         #     Improve writing or keep the same, based on your writing style and above data.\n",
        "\n",
        "#         #     final output:\n",
        "#         #     FOR EACH PAINPOINT, you output  the items below\n",
        "#         #     painpoint:  (improve writing, use less metaphors and be more technical should be 8 to 12 words long)\n",
        "#         #     description: (improve writing , should be 3 to 4 sentences long)\n",
        "#         #     link: (UNCHAGED FROM THE ORIGINAL, OUTPUT THE EXACT SAME LINK, UNCHAGED FROM THE ORIGINAL, OUTPUT THE EXACT SAME LINK)\n",
        "#         #     source: (improve writing: only output the platofrm or site name of which the info is found)\n",
        "#         #     Idea: (improve writing, should be  8 to 15 words long, clearly states the idea and how it would directly solve the problem)\n",
        "#         #     Features: (improve writing, FOR EACH FEATURE, should expand a little bit on this feature should be 8-15 words long)\n",
        "\n",
        "#         #     \"\"\"\n",
        "#         q =  \"\"\"\n",
        "#             OUTPUT FORMAT:\n",
        "#             - Product Name: xxxx\n",
        "#             - Price: $xx.xx\n",
        "#             - Details: xxxxxx\n",
        "#             - Link: (UNCHAGED FROM THE ORIGINAL, OUTPUT THE EXACT SAME LINK, UNCHAGED FROM THE ORIGINAL, OUTPUT THE EXACT SAME LINK)\n",
        "#             \"\"\"\n",
        "\n",
        "#         # initialize chain\n",
        "#         t = ChatPromptTemplate.from_messages ([\n",
        "#             (\"system\",p_system_message_improve_writing),\n",
        "#             (\"human\",q)\n",
        "#             ])\n",
        "\n",
        "#         ideas_chain = t| llm\n",
        "#         out = ideas_chain.invoke({\"input_data\", pp_batch})\n",
        "\n",
        "\n",
        "#         print (f\"*** Improve Writing Batch {i + 1} Complete! ***\")\n",
        "\n",
        "#         # append to ideas_tempt list\n",
        "#         improved_content_temp_list.append (out.content)\n",
        "#         print (f\"batch {i + 1 } content:\",out.content)\n",
        "\n",
        "#     print (\"*** Improve Writng complete ***\")\n",
        "\n",
        "#      # Get today's date in the format YYYY-MM-DD\n",
        "#     today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#     # Construct the filename with today's date\n",
        "#         # Create directory if it doesn't exist\n",
        "#     dir_name = 'raw_improved_content'\n",
        "#     if not os.path.exists(dir_name):\n",
        "#         os.makedirs(dir_name)\n",
        "\n",
        "#     # Define the path for the output file\n",
        "#     file_path = os.path.join(dir_name, f'raw_improved_content_{today_date}.txt')\n",
        "\n",
        "#     # Writing the pain points to the text file\n",
        "#     with open(file_path, 'w') as file:\n",
        "#         for idea in improved_content_temp_list:\n",
        "#             file.write(idea + '\\n')  # Write each pain point on a new line\n",
        "\n",
        "#     print(f\"***Improved Painpoints Content saved to '{file_path}' ***\")\n",
        "\n",
        "#     return improved_content_temp_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvm4KJd6Q8Iq",
        "outputId": "99624a3a-9e0c-42ef-b766-1a907993b520"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'improve_writing_ol' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m improved_content_temp_list\n\u001b[0;32m----> 2\u001b[0m ideas \u001b[38;5;241m=\u001b[39m \u001b[43mimprove_writing_ol\u001b[49m (improved_content_temp_list,main_content_temp_list)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'improve_writing_ol' is not defined"
          ]
        }
      ],
      "source": [
        "improved_content_temp_list\n",
        "ideas = improve_writing_ol (improved_content_temp_list,main_content_temp_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuyCLlJQ8Ir"
      },
      "source": [
        "## Ideas Pipline -> with Ideas Agent - > deprecated\n",
        "IMPORTANT -> the online ideas generation pipeline leads to halucination of ideas, and is deprecated.Save for documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XkbMX5bQ8Ir"
      },
      "source": [
        "Initialize agent function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVoNeQt2Q8Ir"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRED5F1uQ8Ir"
      },
      "outputs": [],
      "source": [
        "# def init_ideas_agent (input_data_batch: list,memory_llm,agent_llm):\n",
        "# # Define memory for this agent\n",
        "#     ideas_memory = ConversationBufferMemory (\n",
        "#         memory_key=\"ideas_memory\",\n",
        "#         return_messages=True,\n",
        "#         llm=memory_llm,\n",
        "#         max_tokens=1000\n",
        "#     )\n",
        "#     ## be careful\n",
        "#     ideas_memory.clear()\n",
        "\n",
        "#     empty_system_message_ideas = SystemMessagePromptTemplate.from_template(p_system_message_ideas_offline)\n",
        "\n",
        "#     system_message_ideas =empty_system_message_ideas.format(input_data=input_data_batch)\n",
        "\n",
        "#     agent_kwargs = {\n",
        "#         \"extract_prompt_messages\" : [MessagesPlaceholder(variable_name=\"ideas_memory\")],\n",
        "#         \"system_message\" : system_message_ideas,\n",
        "#     }\n",
        "\n",
        "#     # Create Agent\n",
        "\n",
        "#     # initialize agent\n",
        "#     ideas_agent = initialize_agent (\n",
        "#         tools=[],\n",
        "#         llm=agent_llm,\n",
        "#         agent=AgentType.OPENAI_FUNCTIONS,\n",
        "#         verbose=True,\n",
        "#         agent_kwargs=agent_kwargs,\n",
        "#         memory=ideas_memory,\n",
        "#     )\n",
        "#     print (\"Ideas Agent Intialized\")\n",
        "#     return ideas_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOjNaduxQ8Ir"
      },
      "outputs": [],
      "source": [
        "# ideas_temp_list= []\n",
        "# ideas = get_ideas (ideas_temp_list, painpoints_temp_list,memory_llm=gpt4,agent_llm=pplx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiO0Qu_AQ8Ir"
      },
      "outputs": [],
      "source": [
        "# ## The main function to get ideas\n",
        "# def get_ideas (ideas_temp_list:list, painpoints_temp_list: list ,memory_llm = gpt3, agent_llm = pplx):\n",
        "\n",
        "#     for i, pp_batch in enumerate (painpoints_temp_list):  # need to add to this\n",
        "#         count  = 1\n",
        "#         # initialize agaent\n",
        "#         ideas_agent = init_ideas_agent(pp_batch,memory_llm=memory_llm,agent_llm=agent_llm)\n",
        "\n",
        "#         # TODO: run ai\n",
        "#         q = \"\"\"\n",
        "#             FOR EACH PAINPOINTS\n",
        "#             1. Generate 1 specific saas idea Solution that directly solves the problem\n",
        "#             2. Generate 3 key features FOR EACH saas ideas generated  Output format :(8-15 word description of each features)\n",
        "#             3. generate 2,  1 to 2 word tags for this item describing their category or domain\n",
        "\n",
        "#             FINAL OUTPUT FORMAT (OUTPUT IN BELOWR FORMAT AND NOTHING ELSE:\n",
        "#             painpoint: xxxx  (8 to 12 words)\n",
        "#             source: xxx (ONLY IF APPLICABLE, IF NOT: \"\")\n",
        "#             link: xxx(ONLY IF APPLICABLE, IF NOT: \"\")\n",
        "#             description: xxxxxx (3 sentences)\n",
        "#             idea: (6-10 word title of the idea; WRITING STYLE: Technical, Prfoessional, Tech Blogger)\n",
        "#             features: xxx , xxx , xxx (8-15 word description of each features,vary;WRITING STYLE: Technical, Prfoessional, Tech Blogger)\n",
        "#             tags: xxx, xx\n",
        "#             \"\"\"\n",
        "#         out = agent.invoke({\"input\": q})\n",
        "#         print (f\"*** Idea Generate Batch {i + 1} Complete! ***\")\n",
        "\n",
        "#         # append to ideas_tempt list\n",
        "#         ideas_temp_list.append (out[\"output\"])\n",
        "\n",
        "#     print (\"*** Ideas Generation Complete ***\")\n",
        "\n",
        "#      # Get today's date in the format YYYY-MM-DD\n",
        "#     today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#     # Construct the filename with today's date\n",
        "#         # Create directory if it doesn't exist\n",
        "#     dir_name = 'ideas_raw'\n",
        "#     if not os.path.exists(dir_name):\n",
        "#         os.makedirs(dir_name)\n",
        "\n",
        "#     # Define the path for the output file\n",
        "#     file_path = os.path.join(dir_name, f'ideas_{today_date}.txt')\n",
        "\n",
        "#     # Writing the pain points to the text file\n",
        "#     with open(file_path, 'w') as file:\n",
        "#         for idea in ideas_temp_list:\n",
        "#             file.write(idea + '\\n')  # Write each pain point on a new line\n",
        "\n",
        "#     print(f\"*** Painpoints saved to '{file_path}' ***\")\n",
        "\n",
        "#     return ideas_temp_list\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "adigen_conda_venv_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vSq0Fcl3Q8Iq"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}